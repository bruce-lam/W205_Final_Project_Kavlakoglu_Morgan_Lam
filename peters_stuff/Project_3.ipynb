{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 Report: Peter Morgan, Bruce Lam, Eda Kavlakoglu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Modules\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "My summary blah blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repository Description\n",
    "bulleted list describing all files that exist in repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Pipeline Breakdown\n",
    "\n",
    "### Set Up\n",
    "Below we will describe in detail how the pipeline is spun up. First, we need to change into the w205 directory and clone the project repository. This is done using the commands below.\n",
    "\n",
    "```console\n",
    "cd ~/w205\n",
    "git clone https://github.com/mids-w205-martin-mims/project-3-superpeter55.git\n",
    "```\n",
    "\n",
    "Next, we must move into our new directory and create a new branch \"assignemnt\" and switch over to that branch. This is done using the commands below. We also include the git status command and its output to ensure that we are on the assignment branch as expected. As you can see, we successfully created and switched to the assignment branch\n",
    "\n",
    "```console\n",
    "cd project-3-superpeter55\n",
    "git branch assignment\n",
    "git checkout assignment\n",
    "git status\n",
    "```\n",
    "```console\n",
    "On branch assignment\n",
    "```\n",
    "\n",
    "The next step is going to be copying our docker-compose file from the week 13 content so we can run our docker images. This is done using the command below. In this case, the period \".\" at the end of the command pastes the copied file in the current directory. Once this is copied in, there are a few changes we have to make to ensure our docker containers run properly. First, we must comment out the two lines in the cloudera service that read \"ports:\" and \"8888:8888\". These lines are already commented out in the spark service in the docker-compose file and must be uncommented. This insures that we can run a jupyter notebook with pyspark in port 8888.\n",
    "\n",
    "```console\n",
    "cp ~/w205/course-content/13-Understanding-Data/docker-compose.yml .\n",
    "```\n",
    "\n",
    "Now that we have copied in the docker-compose file and made the necessary modifications, we will spin up our cluster using the command below. The -d at the end runs the containers in the background and we are able to continue to use our console. Next we will use docker-compose ps to make sure all the containers are up. The command lists all the containers and some basic information on them. Since all the containers say they are in the state \"up\", we will proceed.\n",
    "\n",
    "```console\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "Now that our cluster is running, we will create our kafka topic. We will use the exec command on kafka to execute a single kafka command. In this case we run the kafka-topics command with the --create option to create a topic. The --topic option is used to name the topic \"events\". I chose to name this topic events because we are collecting game event data for this project. We use the --partitions option to specify that we only need one partition. The --if-not-exists option is to prevent two topics with the same name being created. The --zookeeper option is specifying which port we would like to connect our topic to. To ensure this command is successful, the console will output \"Created topic events\".\n",
    "\n",
    "```console\n",
    "docker-compose exec kafka kafka-topics --create --topic events --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n",
    "\n",
    "Moving on we will set up the game_api.py file which will define what type of events we can perform. To start, we will copy the game_api.py file from week 12 and then make some modifications. This is done using the command below.\n",
    "\n",
    "```console\n",
    "cp ~/w205/course-content/12-Querying-Data-II/game_api.py .\n",
    "```\n",
    "\n",
    "The game_api.py file from week 12 does not have functions for buy a sword or join guild so we will need to create them. Some neat functions of this project are that we give the user the option to buy a sword or buy a sharp sword. We also have 3 guilds the user can join to represent each team member on this project. There is the Morgan guild, the Lam guild, and the Kavlakoglu guild. There are separate events for each of these guilds which will join that specific guild while the generic join_guild event will randomly select a guild to join.\n",
    "\n",
    "The functions added to the game_api.py file are shown below. These functions are very similar to the function that already exists in the game_api.py file named purchase_a_sword(). The first line of each function specifies the path that will need to be specified to trigger each event. In this case, we are specifying /buy_a_sword or /purchase_a_sharp_sword to trigger the a buy sword event. To join a guild, we specify /join_guild to join a random guild or the specific guild paths to choose which guild to join. Each of these functions defines a dictionary of metadata. All events have an event type. For buy_sword events, we specify the sword_type which is either normal or sharp. For join_guild events, we specify the guild_name, and the assignment_type which says if the guild was joined randomly or if we chose which guild we were joining. The log_to_kafka step sends the event and event metadata to the kafka topic \"events\". Finally the return statement returns a nice message to the user that tells them what happened. \n",
    "\n",
    "```python\n",
    "@app.route(\"/buy_a_sword\")\n",
    "def buy_a_sword():\n",
    "    buy_sword_event = {'event_type': 'buy_sword',\n",
    "                       'sword_type': 'normal'}\n",
    "    log_to_kafka('events', buy_sword_event)\n",
    "    return \"Sword Bought!\\n\"\n",
    "\n",
    "@app.route(\"/join_guild\")\n",
    "def join_guild():\n",
    "    guilds = ['Morgan','Lam','Kavlakoglu']\n",
    "    guild = random.choice(guilds)\n",
    "    join_guild_event = {'event_type': 'join_guild',\n",
    "                        'guild_name': guild,\n",
    "                        'assignment_type': 'random'}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined \" + guild + \" Guild!\\n\"\n",
    "\n",
    "@app.route(\"/purchase_a_sharp_sword\")\n",
    "def purchase_a_sharp_sword():\n",
    "    purchase_sword_event = {'event_type': 'buy_sword',\n",
    "                            'sword_type': 'sharp'}\n",
    "    log_to_kafka('events', purchase_sword_event)\n",
    "    return \"Sharp Sword Purchased!\\n\"\n",
    "\n",
    "@app.route(\"/join_guild_morgan\")\n",
    "def join_guild_morgan():\n",
    "    join_guild_event = {'event_type': 'join_guild',\n",
    "                        'guild_name': 'Morgan',\n",
    "                        'assignment_type': 'manual'}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined Morgan Guild!\\n\"\n",
    "\n",
    "@app.route(\"/join_guild_lam\")\n",
    "def join_guild_lam():\n",
    "    join_guild_event = {'event_type': 'join_guild',\n",
    "                        'guild_name': 'Lam',\n",
    "                        'assignment_type': 'manual'}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined Lam Guild!\\n\"\n",
    "\n",
    "@app.route(\"/join_guild_kavlakoglu\")\n",
    "def join_guild_kavlakoglu():\n",
    "    join_guild_event = {'event_type': 'join_guild',\n",
    "                        'guild_name': 'Kavlakoglu',\n",
    "                        'assignment_type': 'manual'}\n",
    "    log_to_kafka('events', join_guild_event)\n",
    "    return \"Joined Kavlakoglu Guild!\\n\"\n",
    "```\n",
    "\n",
    "Now that we have updated our game_api.py file, we can run our flask app through the game_api.py file. The code to run our flask app already exists in the game_api file so we just need to run the file and specify the host which in this case is 0.0.0.0. \n",
    "\n",
    "```console\n",
    "docker-compose exec mids env FLASK_APP=/w205/project-3-superpeter55/game_api.py flask run --host 0.0.0.0\n",
    "```\n",
    "\n",
    "Note that since the flask app is now running in the window. We will need to open a new terminal to perform commands. We do this and then switch into the proper directory using the commands below.\n",
    "\n",
    "```console\n",
    "cd w205\n",
    "cd project-3-superpeter55/\n",
    "```\n",
    "\n",
    "The next task we would like to perform is spinning up a jupyter notebook with pyspark. Before we actually spin up the notebook, we must create a symbolic link so that we can access our mounted w205 directory. This is done using the commands below. The first line uses docker-compose exec to open up a spark bash shell. The second line creates link using the ln command and specifies the -s option to create a symbolic link. The third line exits and returns us to the shell.\n",
    "\n",
    "```console\n",
    "docker-compose exec spark bash\n",
    "ln -s /w205 w205\n",
    "exit \n",
    "```\n",
    "\n",
    "Now we are ready to spin up our notebook using pyspark. This is done in the command below. Once again we are using docker-compose exec to run a single spark command. The spark command sets up a notebook in port 8888 and ip address 0.0.0.0. After this command we replace 0.0.0.0 with our local machine ip address and paste the link in our browser to access our notebooks.\n",
    "\n",
    "```console\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "```\n",
    "\n",
    "Now that we are in our notebook, we want to check that the spark session and spark context are accessible in the notebook. The two code cells below show that spark is properly running in this instance and we are ready to proceed. We will move forward to testing our pipeline in batch mode and generating data with apache bench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.31.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f25940b9c50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.31.0.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our pipeline, we will use Apache Bench to generate events and run them through the pipeline. To test our pipeline, we will generate 3 events of each type from 2 separate users which we will call, Player_1 and Player_2. Once again, we will use docker compose exec to excecute Apache Bench commands. The commands we will use to generate our data are listed below. Each event is very similar in that it call \"ab\" which means Apache Bench. The -n argument specifies how many times we would like to call this event which we specify as 3 for every single call to apache bench. The -H argument specifies the host or user which we have decided to specify either Player_1 or Player_2. Finally, the path at the end specifies which event from our game_api.py file will be called. I have decided to call every event in the api 3 times with each user to ensure that our test data covers all possible actions a user can take. Since we have 6 events in our api and 2 users generating test data, it makes sense that we have 12 calls to apache bench specified below. \n",
    "\n",
    "```console\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/purchase_a_sharp_sword\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/purchase_a_sharp_sword\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/buy_a_sword\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/buy_a_sword\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/join_guild\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/join_guild\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/join_guild_morgan\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/join_guild_morgan\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/join_guild_lam\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/join_guild_lam\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_2\" http://localhost:5000/join_guild_kavlakoglu\n",
    "docker-compose exec mids ab -n 3 -H \"Host: Player_1\" http://localhost:5000/join_guild_kavlakoglu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@udf('boolean')\n",
    "def is_purchase(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'buy_sword':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_join(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'join_guild':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "@udf('boolean')\n",
    "def is_default(event_as_json):\n",
    "    event = json.loads(event_as_json)\n",
    "    if event['event_type'] == 'default':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_events = spark \\\n",
    "        .read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .option(\"endingOffsets\", \"latest\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "| key|               value| topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     0|2021-12-08 03:32:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     1|2021-12-08 03:32:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     2|2021-12-08 03:32:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     3|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     4|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     5|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     6|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     7|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     8|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|     9|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    10|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    11|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    12|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    13|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    14|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    15|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    16|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|events|        0|    17|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    18|2021-12-08 03:33:...|            0|\n",
      "|null|[7B 22 48 6F 73 7...|events|        0|    19|2021-12-08 03:33:...|            0|\n",
      "+----+--------------------+------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "purchase_events = raw_events \\\n",
    "        .select(raw_events.value.cast('string').alias('raw'),\n",
    "                raw_events.timestamp.cast('string')) \\\n",
    "        .filter(is_purchase('raw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 raw|           timestamp|\n",
      "+--------------------+--------------------+\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchase_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_purchase_events = purchase_events \\\n",
    "        .rdd \\\n",
    "        .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "        .toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- sword_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_purchase_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|event_type|sword_type|           timestamp|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_purchase_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_purchase_events \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet('/tmp/purchases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 raw|           timestamp|\n",
      "+--------------------+--------------------+\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:33:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:34:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:34:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:34:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:34:...|\n",
      "|{\"event_type\": \"j...|2021-12-08 03:34:...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_events = raw_events \\\n",
    "        .select(raw_events.value.cast('string').alias('raw'),\n",
    "                raw_events.timestamp.cast('string')) \\\n",
    "        .filter(is_join('raw'))\n",
    "        \n",
    "join_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- assignment_type: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- guild_name: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_join_events = join_events \\\n",
    "        .rdd \\\n",
    "        .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "        .toDF()\n",
    "        \n",
    "extracted_join_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|assignment_type|event_type|guild_name|           timestamp|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|Kavlakoglu|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|Kavlakoglu|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_join_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_join_events \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet('/tmp/joins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 raw|           timestamp|\n",
      "+--------------------+--------------------+\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:32:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:32:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:32:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "|{\"Host\": \"Player_...|2021-12-08 03:33:...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "default_events = raw_events \\\n",
    "        .select(raw_events.value.cast('string').alias('raw'),\n",
    "                raw_events.timestamp.cast('string')) \\\n",
    "        .filter(is_default('raw'))\n",
    "        \n",
    "default_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Accept: string (nullable = true)\n",
      " |-- Host: string (nullable = true)\n",
      " |-- User-Agent: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_default_events = default_events \\\n",
    "        .rdd \\\n",
    "        .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \\\n",
    "        .toDF()\n",
    "        \n",
    "extracted_default_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|event_type|           timestamp|\n",
      "+------+--------+---------------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "+------+--------+---------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_default_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_default_events \\\n",
    "        .write \\\n",
    "        .mode('overwrite') \\\n",
    "        .parquet('/tmp/defaults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|event_type|sword_type|           timestamp|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases = spark.read.parquet('/tmp/purchases')\n",
    "purchases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "purchases.registerTempTable('purchases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|event_type|sword_type|           timestamp|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|     sharp|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3| buy_sword|    normal|2021-12-08 03:33:...|\n",
      "+------+--------+---------------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purchases_player1 = spark.sql(\"select * from purchases where host='Player_1'\")\n",
    "purchases_player1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|assignment_type|event_type|guild_name|           timestamp|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         random|join_guild|Kavlakoglu|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|Kavlakoglu|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joins = spark.read.parquet('/tmp/joins')\n",
    "joins.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joins.registerTempTable('joins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|assignment_type|event_type|guild_name|           timestamp|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|Kavlakoglu|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         random|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|    Morgan|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|       Lam|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|         manual|join_guild|Kavlakoglu|2021-12-08 03:34:...|\n",
      "+------+--------+---------------+---------------+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joins_player2 = spark.sql(\"select * from joins where host='Player_2'\")\n",
    "joins_player2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+----------+--------------------+\n",
      "|Accept|    Host|     User-Agent|event_type|           timestamp|\n",
      "+------+--------+---------------+----------+--------------------+\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_1|ApacheBench/2.3|   default|2021-12-08 03:32:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "|   */*|Player_2|ApacheBench/2.3|   default|2021-12-08 03:33:...|\n",
      "+------+--------+---------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "defaults = spark.read.parquet('/tmp/defaults')\n",
    "defaults.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Streaming Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def purchase_sword_event_schema():\n",
    "    \"\"\"\n",
    "    root\n",
    "    |-- Accept: string (nullable = true)\n",
    "    |-- Host: string (nullable = true)\n",
    "    |-- User-Agent: string (nullable = true)\n",
    "    |-- event_type: string (nullable = true)\n",
    "    |-- timestamp: string (nullable = true)\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"Accept\", StringType(), True),\n",
    "        StructField(\"Host\", StringType(), True),\n",
    "        StructField(\"User-Agent\", StringType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"sword_type\", StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_events = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "        .option(\"subscribe\", \"events\") \\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sword_purchases = raw_events \\\n",
    "        .filter(is_purchase(raw_events.value.cast('string'))) \\\n",
    "        .select(raw_events.value.cast('string').alias('raw_event'),\n",
    "                raw_events.timestamp.cast('string'),\n",
    "                from_json(raw_events.value.cast('string'),\n",
    "                          purchase_sword_event_schema()).alias('json')) \\\n",
    "        .select('raw_event', 'timestamp', 'json.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o313.start.\n: java.lang.IllegalStateException: Cannot start query with id 41bdefa5-7dce-4628-b88f-26e229496251 as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:300)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:282)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e0ba03465c9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msword_purchases\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/tmp/checkpoints_for_sword_purchases\"\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/tmp/sword_purchases\"\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessingTime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"10 seconds\"\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/spark-2.2.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o313.start.\n: java.lang.IllegalStateException: Cannot start query with id 41bdefa5-7dce-4628-b88f-26e229496251 as another query with same id is already active. Perhaps you are attempting to restart a query from checkpoint that is already active.\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:300)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:282)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "sink = sword_purchases \\\n",
    "        .writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"checkpointLocation\", \"/tmp/checkpoints_for_sword_purchases\") \\\n",
    "        .option(\"path\", \"/tmp/sword_purchases\") \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Events\n",
    "\n",
    "Open third terminal, run while loop to continuously generate events. In this case, each player is going to perform every event the api has to offer once every 10 seconds. Once again, Apache Bench is used to generate the events. The only difference from the streaming version from the batch version is that we are specifying each event only happen once and we are also going to run each bench command once every 10 seconds so we are continuously streaming data.\n",
    "\n",
    "```console\n",
    "while true; do\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/purchase_a_sharp_sword\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/purchase_a_sharp_sword\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/buy_a_sword\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/buy_a_sword\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/join_guild\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/join_guild\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/join_guild_morgan\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/join_guild_morgan\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/join_guild_lam\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/join_guild_lam\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_2\" http://localhost:5000/join_guild_kavlakoglu\n",
    "    docker-compose exec mids ab -n 1 -H \"Host: Player_1\" http://localhost:5000/join_guild_kavlakoglu\n",
    "  sleep 10\n",
    "done\n",
    "```\n",
    "\n",
    "Now, we are ready to start Hive. this is done below.\n",
    "\n",
    "```console\n",
    "docker-compose exec cloudera hive\n",
    "```\n",
    "\n",
    "Write to HDFS in parquet format from hive\n",
    "\n",
    "```console\n",
    "create external table if not exists default.sword_purchases (Accept string, Host string, `User-Agent` string, event_type string, sword_size string, timestamp string) stored as parquet location '/tmp/sword_purchases'  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Analytics Questions\n",
    "\n",
    "Use sql in presto\n",
    "how many events are there?\n",
    "how many events from user1/user2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sink.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
